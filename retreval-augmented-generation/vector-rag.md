# Vector RAG

## 개념

### RAG(Retrieval-Augmented Generation)?

- LLM 모델이 뱉는 걸 그대로 활용하기보다 독자적인 지식 베이스를 구축하고 그걸 토대로 응답토록 하는 것.
- 할루시네이션 방지.
- 적시성 문제 해결
- 데이터 보안, 데이터 출처 문제
- 응답제한 문제 등...

### Vector RAG

- Vector DB 기반 RAG
- Vector 데이터 기반 유사도를 측정하여 보다 더 세분화 된 지식을 검색한다.

<br>

## 활용 Retrieval

### 가상 질문 인덱스 검색(Hypothetical Query Embeddings, HyQE)

데이터베이스에 저장된 데이터 청크에 대해 하나 이상의 가상 질문을 미리 생성해둔다.
나중에 검색 단계에 정보를 제공하는데 활용할 수 있다.

[원본문서] - [청크1, 청크2, 청크3] - [청크1의 가상질문, 청크2의 가상질문, ...] <- 사용자 질의

### 계층 인덱스

청크를 더 잘게잘게 자른다. 이분트리 느낌.

[원본문서] - [청크1, 청크2, 청크3] - [청크1-1, 청크1-2, 청크2-1, 청크2-2, 청크2-3, ...] <- 사용자 질의

### 가상 답변 유사도 조회

LLM 이 사용자 질의에 대한 가상 답변을 생성한 후, 그 답변을 Vector DB 에 던져 유사도를 측정한다.

[원본문서] - [청크] <- [사용자 질의의 가상답변] <- 사용자 질의

### Small To Big 검색

유사도 검색시 청크 단위가 크면 방해가 된다. 이를 더 작은 청크 단위로 검색을 진행한다.
그를 통해 검색된 결과 청크를 LLM 에게 전달한다. LLM 은 많은 정보를 전달해주어야 답변 생성에 유리하기 때문이다.

- Parent to Child: 부모 청크를 찾아서 던지는 방법
  - 전체 청크를 전달하면 LLM 이 더 많은 정보를 가지고 답변을 생성할 수 있다.
  - 그러나 비용과 속도 측면 문제가 존재한다.
- Sentence window: 한 문장으로 검색 후 문단을 던지는 방법
  - 문단을 넘어 인접한 청크에 담긴 내용도 함께 전달한다.

### Atomic Chunking(원자적 청크 분류)

청크의 임베딩 값은 청크 속 정보들의 평균 값이다. 만약 이들의 편차가 클 경우 쿼리시 제대로 된 정보를 얻기 힘들다.
이를 개선하기 위한 방법 중 하나.

- 기본 컨셉은 Small To Big 과 같다.
- Small 청크를 원자(1개의 뜻을 가진 문장) 단위로 갖도록 구성한다.
- 원자 단위와 질의의 길이를 일치시키면서 검색 성능을 향상한다.

### Hybrid Vector

정확성을 목표로 하는 키워드 검색의 벡터값과 유사도를 목표로 하는 시멘틱 검색의 벡터값을 각각 나누어 저장하고,
질의시 2개 모두를 검색하여 종합 결과를 내보내는 형태.

### Metadata Filtering

1. Pre-filtering
   - 유사도 검색에서는 데이터량이 많으면 방해가 된다.
   - 이를 미리 필터링하여 검색 범위를 줄인다.
   - 데이터량이 많으면 검색 결과가 느릴 수 있겠지만, 정확도가 높은 K 를 반환한다는 것이 큰 의의를 갖는다.
   - 검색 품질이 매우 높은 것.
2. Post-filtering
   - 검색 결과를 받은 후, 추가적인 필터링을 진행한다.
   - K 개수가 부족한 경우 정확성과 안정성이 떨어진다.
   - 대신 검색 속도가 선택의 폭보다 중요한 경우
   - 데이터가 초대용량이라면 어쩔 수 없이 검색 속도가 더 중요할 수 밖에 없어진다.

### Collection 분리 분산

당연히 샤딩하면 더 좋겠죠...
